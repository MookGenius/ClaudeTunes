# üéØ Screenshot Hybrid Integration Strategy
**Combining GT7 Data Logger Screenshots + UDP Telemetry + Phase 2 Metrics**

---

## Executive Summary

This document outlines the strategy for integrating GT7 Spec III Data Logger screenshots with UDP telemetry and ClaudeTunes Phase 2 analysis to create a **spatially-indexed, physics-validated, professionally-diagnosed telemetry system**.

### The Core Innovation

```
GT7 Screenshots (Spatial Context)
    √ó
UDP 60Hz Telemetry (Continuous Physics)
    √ó
Phase 2 Metrics (Professional Diagnostics)
    =
Corner-Specific Physics Analysis with Validation Loop
```

### Expected Impact

- **Precision:** Lap-level ‚Üí Corner-level problem diagnosis
- **Accuracy:** General recommendations ‚Üí Specific physics-validated fixes
- **Validation:** Predictions tracked session-to-session for learning
- **Value:** $0 solution rivals $50,000 MoTeC systems

---

## Table of Contents

1. [Problem Statement](#problem-statement)
2. [Data Sources](#data-sources)
3. [Architecture Overview](#architecture-overview)
4. [Screenshot Data Extraction](#screenshot-data-extraction)
5. [Correlation Engine](#correlation-engine)
6. [Corner-Specific Analysis](#corner-specific-analysis)
7. [Integration with ClaudeTunes v2](#integration-with-claudetunes-v2)
8. [Implementation Roadmap](#implementation-roadmap)
9. [Technical Specifications](#technical-specifications)
10. [Expected Outputs](#expected-outputs)
11. [Success Metrics](#success-metrics)
12. [Dependencies](#dependencies)

---

## Problem Statement

### Current Limitations

**UDP Telemetry Alone (gt7_2rTEST.py):**
- ‚úÖ Complete physics data (60Hz)
- ‚úÖ Phase 2 professional metrics (damper histograms, heave, platform dynamics)
- ‚ùå **No spatial context** - can't pinpoint WHERE on track problems occur
- ‚ùå No GT7 official validation of lap times
- ‚ùå No reference lap comparison data

**GT7 Screenshots Alone:**
- ‚úÖ Official lap times and sector splits
- ‚úÖ Visual delta ribbons (red/green time loss indicators)
- ‚úÖ Track position context
- ‚ùå **No deep physics data** - just visual graphs
- ‚ùå No professional diagnostic metrics
- ‚ùå Requires manual interpretation

### The Solution

**Combine both data sources** to get spatial context + physics depth + professional diagnostics.

---

## Data Sources

### 1. GT7 Spec III Data Logger Screenshots

**Format:** PNG/JPG screenshots from GT7 in-game data logger

**Extractable Data:**

#### Level 1: Text (OCR)
```json
{
  "best_lap": "1:29.374",
  "sectors": ["0:28.412", "0:32.147", "0:28.815"],
  "reference_delta": "-2.174s",
  "car_name": "2001 Chevrolet Corvette Z06",
  "track_name": "Laguna Seca Raceway",
  "max_speed": "145 mph",
  "min_speed": "42 mph"
}
```

**Extraction Accuracy:** 95-99%

#### Level 2: Graph Digitization (Vision AI)
```json
{
  "speed_trace": {
    "braking_zones": [
      {"position_pct": 15, "entry_speed": 145, "exit_speed": 68, "steepness": "sharp"},
      {"position_pct": 60, "entry_speed": 112, "exit_speed": 42, "steepness": "sharp"}
    ],
    "acceleration_zones": [...]
  },
  "throttle_brake_trace": {
    "brake_applications": [
      {"position_pct": 14, "peak_pressure": 95, "duration": 2.1}
    ]
  }
}
```

**Extraction Accuracy:** 80-90%

#### Level 3: Visual Pattern Recognition (Vision AI)
```json
{
  "delta_ribbon": {
    "slow_sections": [
      {
        "location": "Turn 6 (Corkscrew)",
        "track_position_pct": 60,
        "estimated_time_loss": "0.4s",
        "visual_severity": "dark red",
        "phase": "corner_entry"
      },
      {
        "location": "Turn 11",
        "track_position_pct": 85,
        "estimated_time_loss": "0.3s",
        "visual_severity": "red",
        "phase": "corner_exit"
      }
    ],
    "fast_sections": [...]
  }
}
```

**Extraction Accuracy:** 85-95% for location identification

### 2. UDP Telemetry (gt7_1r.py)

**Format:** CSV files per lap, 60Hz sampling

**Data Available:**
- Continuous suspension position (4 corners)
- Tire temperatures and slip ratios
- Speed, throttle, brake, steering inputs
- Body pitch, roll, yaw
- Engine data (RPM, temps, boost)
- Timestamps (current_lap_time)

**Sample Rate:** 60 Hz (16.67ms per sample)

**Storage:** ~10 laps √ó 90s √ó 60Hz = 54,000 data points per session

### 3. Phase 2 Analysis (gt7_2rTEST.py)

**Output Format:** JSON analysis file

**Phase 2 Metrics Available:**

```json
{
  "damper_analysis": {
    "histograms": {
      "fl": {"-300": 0.5, "-275": 1.2, ..., "300": 0.3},
      "fr": {...}, "rl": {...}, "rr": {...}
    },
    "diagnostics": {
      "fl": {
        "center_peak_percent": 5.2,
        "compression_percent": 68.5,
        "rebound_percent": 31.5,
        "symmetry_deviation": 37.0,
        "assessment": "NEEDS ATTENTION - Dampers too soft",
        "warnings": ["Low center peak - dampers may be too soft"]
      }
    },
    "max_velocity": {
      "fl_comp": -520, "fl_reb": 480,
      "fr_comp": -505, "fr_reb": 465
    }
  },
  "platform_dynamics": {
    "avg_heave_mm": 62.4,
    "heave_range_mm": 85.3,
    "pitch_stability": 0.067,
    "roll_stability": 0.045
  }
}
```

**Analysis Scope:** Full lap or windowed segment

---

## Architecture Overview

### System Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    GT7 RACING SESSION                        ‚îÇ
‚îÇ  (Player drives 5-10 laps at Laguna Seca)                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                               ‚îÇ
             ‚ñº                               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ GT7 Data Logger‚îÇ            ‚îÇ Raspberry Pi UDP ‚îÇ
    ‚îÇ (Spec III)     ‚îÇ            ‚îÇ Telemetry Logger ‚îÇ
    ‚îÇ                ‚îÇ            ‚îÇ (gt7_1r.py)      ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                               ‚îÇ
             ‚îÇ Screenshots                   ‚îÇ CSV files
             ‚îÇ (manual capture)              ‚îÇ (automatic)
             ‚ñº                               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ Screenshot     ‚îÇ            ‚îÇ gt7_2rTEST.py    ‚îÇ
    ‚îÇ Extractor      ‚îÇ            ‚îÇ Phase 2 Analysis ‚îÇ
    ‚îÇ (Vision AI)    ‚îÇ            ‚îÇ                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
             ‚îÇ                               ‚îÇ
             ‚îÇ Extracted data                ‚îÇ JSON analysis
             ‚îÇ                               ‚îÇ
             ‚ñº                               ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ     CORRELATION ENGINE (NEW)                    ‚îÇ
    ‚îÇ  ‚Ä¢ Timestamp synchronization                    ‚îÇ
    ‚îÇ  ‚Ä¢ Track position mapping                       ‚îÇ
    ‚îÇ  ‚Ä¢ Spatial ‚Üí Temporal conversion                ‚îÇ
    ‚îÇ  ‚Ä¢ Problem location ‚Üí UDP window extraction     ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ     CORNER-SPECIFIC PHASE 2 ANALYSIS (NEW)      ‚îÇ
    ‚îÇ  ‚Ä¢ Extract UDP packets for problem corner       ‚îÇ
    ‚îÇ  ‚Ä¢ Run Phase 2 metrics on corner window         ‚îÇ
    ‚îÇ  ‚Ä¢ Compare corner vs lap-average metrics        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ     CLAUDETUNES V2 INTEGRATION (ENHANCED)       ‚îÇ
    ‚îÇ  ‚Ä¢ Reads corner-specific analysis               ‚îÇ
    ‚îÇ  ‚Ä¢ Generates targeted recommendations           ‚îÇ
    ‚îÇ  ‚Ä¢ Tracks prediction accuracy                   ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ           HYBRID ANALYSIS REPORT                ‚îÇ
    ‚îÇ  ‚Ä¢ Screenshot + UDP + Phase 2 combined          ‚îÇ
    ‚îÇ  ‚Ä¢ Corner-specific diagnoses                    ‚îÇ
    ‚îÇ  ‚Ä¢ Validated setup recommendations              ‚îÇ
    ‚îÇ  ‚Ä¢ Prediction tracking                          ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Data Flow

1. **Pre-Session:** Start gt7_1r.py UDP capture
2. **During Session:** Drive laps, GT7 records data
3. **Post-Session:** Take GT7 screenshots (3-5 images)
4. **Processing:**
   - gt7_2rTEST.py processes UDP ‚Üí JSON analysis
   - Screenshot extractor processes images ‚Üí JSON data
   - Correlation engine links screenshot locations to UDP windows
   - Corner-specific analyzer runs Phase 2 on problem areas
   - ClaudeTunes v2 generates targeted recommendations
5. **Output:** Hybrid analysis report with corner-specific physics

---

## Screenshot Data Extraction

### Implementation: Vision AI Pipeline

**File:** `gt7_screenshot_extractor.py`

```python
#!/usr/bin/env python3
"""
GT7 Screenshot Data Extractor
Uses Claude Vision API to extract telemetry from GT7 Data Logger screenshots
"""

import anthropic
import base64
import json
from pathlib import Path

class GT7ScreenshotExtractor:
    """Extract telemetry data from GT7 screenshots using multimodal AI"""

    def __init__(self, api_key=None):
        self.client = anthropic.Anthropic(api_key=api_key)

    def extract_all_data(self, screenshot_path: str) -> dict:
        """
        Extract all available data from screenshot
        Returns: Combined extraction result
        """

        # Level 1: Text extraction
        text_data = self.extract_text_data(screenshot_path)

        # Level 2: Graph digitization
        graph_data = self.extract_graph_data(screenshot_path)

        # Level 3: Visual pattern analysis
        pattern_data = self.extract_visual_patterns(screenshot_path)

        return {
            'text_data': text_data,
            'graph_data': graph_data,
            'pattern_data': pattern_data,
            'screenshot_path': screenshot_path
        }

    def extract_text_data(self, screenshot_path: str) -> dict:
        """Extract lap times, sector times, deltas via OCR"""

        prompt = """
        Analyze this GT7 Data Logging screenshot and extract all numeric text data.

        Find and extract:
        1. Best lap time (format: M:SS.mmm or MM:SS.mmm)
        2. Sector 1 time (format: S.mmm or SS.mmm or M:SS.mmm)
        3. Sector 2 time
        4. Sector 3 time
        5. Delta to reference lap (format: -S.mmm or +S.mmm)
        6. Maximum speed achieved (with unit: mph or km/h)
        7. Minimum speed achieved (with unit: mph or km/h)
        8. Car name (full text)
        9. Track name (full text)

        Return ONLY valid JSON with this exact structure:
        {
          "best_lap": "1:29.374",
          "sector_1": "0:28.412",
          "sector_2": "0:32.147",
          "sector_3": "0:28.815",
          "reference_delta": "-2.174",
          "max_speed": "145 mph",
          "min_speed": "42 mph",
          "car_name": "2001 Chevrolet Corvette Z06",
          "track_name": "Laguna Seca Raceway"
        }

        If any value is not visible or unclear, use null.
        Do not include any explanation, only the JSON.
        """

        return self._call_vision_api(screenshot_path, prompt)

    def extract_graph_data(self, screenshot_path: str) -> dict:
        """Extract speed/throttle/brake graph patterns"""

        prompt = """
        Analyze all graphs in this GT7 screenshot.

        For the SPEED TRACE graph:
        1. Identify all major braking zones (sharp downward slopes)
        2. For each braking zone, estimate:
           - Position on track (percentage 0-100%)
           - Entry speed (value from graph)
           - Exit speed (lowest point)
           - Steepness (gradual/moderate/sharp)

        For the THROTTLE/BRAKE graph (if visible):
        3. Identify all brake application points
        4. For each brake point, estimate:
           - Position on track (percentage 0-100%)
           - Peak pressure (0-100% if visible)
           - Duration (approximate seconds)

        Return ONLY valid JSON:
        {
          "speed_trace": {
            "max_speed": 145,
            "min_speed": 42,
            "braking_zones": [
              {
                "position_pct": 15,
                "entry_speed": 145,
                "exit_speed": 68,
                "steepness": "sharp"
              }
            ]
          },
          "throttle_brake_trace": {
            "brake_applications": [
              {
                "position_pct": 14,
                "peak_pressure": 95,
                "duration": 2.1
              }
            ]
          }
        }

        If graphs are not visible, return empty arrays.
        Do not include explanation, only JSON.
        """

        return self._call_vision_api(screenshot_path, prompt)

    def extract_visual_patterns(self, screenshot_path: str) -> dict:
        """Extract delta ribbons and colored problem areas"""

        prompt = """
        Analyze visual comparison overlays in this GT7 screenshot.

        Look for colored delta ribbons or track map segments:
        - GREEN/BLUE sections = time gained (faster than reference)
        - RED/ORANGE sections = time lost (slower than reference)
        - YELLOW sections = neutral (similar to reference)

        For EACH RED/SLOW section (problem area):
        1. Identify the location:
           - Corner name (if labeled: "Turn 6", "Corkscrew", etc.)
           - Track position percentage (0-100%)
        2. Estimate time loss magnitude:
           - If numeric overlay visible, extract exact value
           - If not, estimate from color intensity: "0.1-0.2s" / "0.3-0.5s" / "0.5s+"
        3. Identify the phase (from graph correlation):
           - "corner_entry" if red appears at braking/turn-in
           - "apex" if red at corner middle
           - "corner_exit" if red at throttle application
           - "straight" if red on straights

        Return ONLY valid JSON:
        {
          "slow_sections": [
            {
              "location": "Turn 6 (Corkscrew)",
              "track_position_pct": 60,
              "estimated_time_loss": "0.4s",
              "visual_severity": "dark red",
              "phase": "corner_entry"
            }
          ],
          "fast_sections": [
            {
              "location": "Turn 3-4 Complex",
              "track_position_pct": 35,
              "estimated_time_gain": "0.2s",
              "visual_severity": "green"
            }
          ]
        }

        If no colored overlays visible, return empty arrays.
        Do not include explanation, only JSON.
        """

        return self._call_vision_api(screenshot_path, prompt)

    def _call_vision_api(self, image_path: str, prompt: str) -> dict:
        """Internal: Call Claude Vision API with image + prompt"""

        # Read and encode image
        with open(image_path, 'rb') as f:
            image_data = base64.standard_b64encode(f.read()).decode('utf-8')

        # Determine media type
        ext = Path(image_path).suffix.lower()
        media_type = {
            '.png': 'image/png',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg'
        }.get(ext, 'image/png')

        # Call API
        message = self.client.messages.create(
            model="claude-sonnet-4",
            max_tokens=2048,
            messages=[{
                "role": "user",
                "content": [
                    {
                        "type": "image",
                        "source": {
                            "type": "base64",
                            "media_type": media_type,
                            "data": image_data
                        }
                    },
                    {
                        "type": "text",
                        "text": prompt
                    }
                ]
            }]
        )

        # Parse JSON response
        response_text = message.content[0].text.strip()

        # Handle markdown code blocks
        if response_text.startswith('```'):
            lines = response_text.split('\n')
            response_text = '\n'.join(lines[1:-1])  # Remove ```json and ```

        try:
            return json.loads(response_text)
        except json.JSONDecodeError as e:
            print(f"Warning: Failed to parse JSON response: {e}")
            print(f"Raw response: {response_text}")
            return {}

    def parse_time_to_seconds(self, time_str: str) -> float:
        """Convert "1:29.374" or "0:28.412" or "28.412" to seconds"""
        if not time_str:
            return 0.0

        time_str = time_str.strip()

        if ':' in time_str:
            parts = time_str.split(':')
            minutes = int(parts[0])
            seconds = float(parts[1])
            return minutes * 60 + seconds
        else:
            return float(time_str)


def main():
    """Example usage"""
    import sys

    if len(sys.argv) < 2:
        print("Usage: python gt7_screenshot_extractor.py <screenshot_path>")
        return

    screenshot_path = sys.argv[1]

    print(f"Extracting data from: {screenshot_path}")
    print("=" * 70)

    extractor = GT7ScreenshotExtractor()

    # Extract all data
    data = extractor.extract_all_data(screenshot_path)

    # Save to JSON
    output_path = Path(screenshot_path).with_suffix('.extracted.json')
    with open(output_path, 'w') as f:
        json.dump(data, f, indent=2)

    print(f"\n‚úÖ Extraction complete!")
    print(f"üìÑ Saved to: {output_path}")

    # Display summary
    text = data.get('text_data', {})
    if text:
        print(f"\nüìä Text Data:")
        print(f"  Best Lap: {text.get('best_lap', 'N/A')}")
        print(f"  Sectors: {text.get('sector_1', 'N/A')} | {text.get('sector_2', 'N/A')} | {text.get('sector_3', 'N/A')}")
        print(f"  Delta: {text.get('reference_delta', 'N/A')}")

    patterns = data.get('pattern_data', {})
    slow_sections = patterns.get('slow_sections', [])
    if slow_sections:
        print(f"\nüî¥ Problem Areas Found: {len(slow_sections)}")
        for section in slow_sections:
            print(f"  - {section.get('location', 'Unknown')}: {section.get('estimated_time_loss', 'N/A')} loss")

if __name__ == '__main__':
    main()
```

### Expected Extraction Accuracy

| Data Type | Accuracy | Notes |
|-----------|----------|-------|
| Lap times (text) | 99%+ | Large, clear text |
| Sector times (text) | 98%+ | Clear formatting |
| Max/min speeds (text) | 95%+ | May vary by screenshot quality |
| Delta values (text) | 95%+ | Usually prominent |
| Graph braking zones | 85-90% | Position estimation has variance |
| Brake pressure values | 80-85% | Depends on graph resolution |
| Delta ribbon locations | 90-95% | Color detection is reliable |
| Time loss estimates | 70-80% | Often requires interpretation |

---

## Correlation Engine

### Purpose

Map screenshot spatial data (track position %) to UDP temporal data (timestamps).

**File:** `screenshot_correlation.py`

```python
#!/usr/bin/env python3
"""
Screenshot Correlation Engine
Links GT7 screenshot spatial data to UDP telemetry temporal windows
"""

import json
from pathlib import Path
from typing import List, Dict, Tuple, Optional

class ScreenshotCorrelator:
    """Correlate screenshot locations with UDP telemetry windows"""

    def __init__(self, track_database_path: str = None):
        """
        Initialize correlator

        Args:
            track_database_path: Path to track_database.json with corner metadata
        """
        self.track_database = self._load_track_database(track_database_path)

    def _load_track_database(self, path: str) -> dict:
        """Load track corner database"""
        if path and Path(path).exists():
            with open(path, 'r') as f:
                return json.load(f)

        # Default database with common tracks
        return {
            'Laguna_Seca': {
                'corners': {
                    'Turn_1': {'track_pct': 8, 'type': 'slow', 'name': 'Andretti Hairpin'},
                    'Turn_2': {'track_pct': 15, 'type': 'medium', 'name': 'Turn 2'},
                    'Turn_3': {'track_pct': 22, 'type': 'fast', 'name': 'Turn 3'},
                    'Turn_4': {'track_pct': 28, 'type': 'medium', 'name': 'Turn 4'},
                    'Turn_5': {'track_pct': 35, 'type': 'fast', 'name': 'Turn 5'},
                    'Turn_6': {'track_pct': 60, 'type': 'slow', 'name': 'Corkscrew'},
                    'Turn_7': {'track_pct': 65, 'type': 'slow', 'name': 'Turn 7'},
                    'Turn_8': {'track_pct': 75, 'type': 'fast', 'name': 'Turn 8'},
                    'Turn_9': {'track_pct': 82, 'type': 'medium', 'name': 'Turn 9'},
                    'Turn_10': {'track_pct': 88, 'type': 'slow', 'name': 'Turn 10'},
                    'Turn_11': {'track_pct': 95, 'type': 'fast', 'name': 'Turn 11'}
                },
                'sectors': [
                    {'start': 0, 'end': 32, 'name': 'Sector 1'},
                    {'start': 32, 'end': 68, 'name': 'Sector 2'},
                    {'start': 68, 'end': 100, 'name': 'Sector 3'}
                ]
            }
            # Add more tracks as needed
        }

    def correlate_best_lap(self, screenshot_data: dict, udp_analysis: dict) -> Optional[dict]:
        """
        Find UDP lap matching screenshot best lap time

        Args:
            screenshot_data: Extracted screenshot data
            udp_analysis: Full session analysis from gt7_2rTEST.py

        Returns:
            Matched lap data with correlation metadata
        """

        # Parse screenshot lap time
        text_data = screenshot_data.get('text_data', {})
        best_lap_str = text_data.get('best_lap')

        if not best_lap_str:
            print("Warning: No best lap time found in screenshot")
            return None

        # Convert to seconds
        gt7_time = self._parse_time(best_lap_str)

        # Find closest matching UDP lap
        laps = udp_analysis.get('individual_laps', [])

        if not laps:
            print("Warning: No laps found in UDP analysis")
            return None

        closest_lap = None
        min_diff = float('inf')

        for lap in laps:
            lap_time = lap['lap_summary']['lap_time']
            diff = abs(lap_time - gt7_time)

            if diff < min_diff and diff < 1.0:  # 1.0 second tolerance
                min_diff = diff
                closest_lap = lap

        if not closest_lap:
            print(f"Warning: No UDP lap found within 1.0s of GT7 time {gt7_time:.3f}s")
            return None

        # Calculate correlation quality
        confidence = 1.0 - (min_diff / 1.0)  # 0.0 to 1.0

        return {
            'gt7_official_time': gt7_time,
            'udp_calculated_time': closest_lap['lap_summary']['lap_time'],
            'sync_error_seconds': min_diff,
            'correlation_confidence': confidence,
            'lap_number': closest_lap.get('lap_number', 0),
            'lap_data': closest_lap
        }

    def map_problem_to_udp_window(self,
                                    problem_location: dict,
                                    correlated_lap: dict,
                                    window_seconds: float = 3.0) -> dict:
        """
        Extract UDP data window for a specific problem location

        Args:
            problem_location: Problem area from screenshot (with track_position_pct)
            correlated_lap: Matched lap from correlate_best_lap()
            window_seconds: Time window around problem (¬±seconds)

        Returns:
            UDP data window with problem context
        """

        track_pct = problem_location.get('track_position_pct', 0)
        lap_time = correlated_lap['udp_calculated_time']

        # Convert track position to timestamp
        problem_timestamp = lap_time * (track_pct / 100.0)

        # Calculate window boundaries
        window_start = max(0, problem_timestamp - window_seconds)
        window_end = min(lap_time, problem_timestamp + window_seconds)

        return {
            'problem_location': problem_location,
            'track_position_pct': track_pct,
            'timestamp_center': problem_timestamp,
            'window_start': window_start,
            'window_end': window_end,
            'window_duration': window_end - window_start
        }

    def _parse_time(self, time_str: str) -> float:
        """Convert time string to seconds"""
        if not time_str:
            return 0.0

        time_str = time_str.strip()

        if ':' in time_str:
            parts = time_str.split(':')
            minutes = int(parts[0])
            seconds = float(parts[1])
            return minutes * 60 + seconds
        else:
            return float(time_str)


def correlate_session(screenshot_json: str,
                      udp_analysis_json: str,
                      output_path: str = None) -> dict:
    """
    Main correlation function

    Args:
        screenshot_json: Path to extracted screenshot JSON
        udp_analysis_json: Path to gt7_2rTEST.py output JSON
        output_path: Optional output path for correlation results

    Returns:
        Correlation results with problem windows
    """

    # Load data
    with open(screenshot_json, 'r') as f:
        screenshot_data = json.load(f)

    with open(udp_analysis_json, 'r') as f:
        udp_analysis = json.load(f)

    # Initialize correlator
    correlator = ScreenshotCorrelator()

    # Find best lap match
    print("Correlating best lap...")
    correlated_lap = correlator.correlate_best_lap(screenshot_data, udp_analysis)

    if not correlated_lap:
        print("‚ùå Failed to correlate best lap")
        return None

    print(f"‚úÖ Best lap matched:")
    print(f"   GT7 Official: {correlated_lap['gt7_official_time']:.3f}s")
    print(f"   UDP Calculated: {correlated_lap['udp_calculated_time']:.3f}s")
    print(f"   Sync Error: {correlated_lap['sync_error_seconds']:.3f}s")
    print(f"   Confidence: {correlated_lap['correlation_confidence']:.2%}")

    # Map problem locations to UDP windows
    print("\nMapping problem locations...")
    pattern_data = screenshot_data.get('pattern_data', {})
    slow_sections = pattern_data.get('slow_sections', [])

    problem_windows = []
    for problem in slow_sections:
        window = correlator.map_problem_to_udp_window(problem, correlated_lap)
        problem_windows.append(window)

        print(f"  üî¥ {problem.get('location', 'Unknown')}")
        print(f"     Track position: {problem.get('track_position_pct', 0)}%")
        print(f"     Timestamp: {window['timestamp_center']:.1f}s into lap")
        print(f"     Window: {window['window_start']:.1f}s - {window['window_end']:.1f}s")

    # Compile results
    results = {
        'correlation': correlated_lap,
        'problem_windows': problem_windows,
        'screenshot_data': screenshot_data,
        'session_info': udp_analysis.get('session_info', {})
    }

    # Save if output path provided
    if output_path:
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\n‚úÖ Correlation saved to: {output_path}")

    return results


def main():
    """Example usage"""
    import sys

    if len(sys.argv) < 3:
        print("Usage: python screenshot_correlation.py <screenshot.extracted.json> <telemetry_analysis_v3_TEST.json>")
        return

    screenshot_json = sys.argv[1]
    udp_json = sys.argv[2]
    output_path = sys.argv[3] if len(sys.argv) > 3 else 'correlation_results.json'

    print("Screenshot Correlation Engine")
    print("=" * 70)

    results = correlate_session(screenshot_json, udp_json, output_path)

    if results:
        print("\n" + "=" * 70)
        print("‚úÖ Correlation complete!")

if __name__ == '__main__':
    main()
```

---

## Corner-Specific Analysis

### Purpose

Run Phase 2 metrics on specific corner windows identified by screenshots.

**File:** `corner_specific_analyzer.py`

```python
#!/usr/bin/env python3
"""
Corner-Specific Phase 2 Analyzer
Extracts and analyzes specific corner windows from UDP telemetry
"""

import json
import csv
from pathlib import Path
from typing import List, Dict

# Import Phase 2 analysis functions from gt7_2rTEST.py
import sys
sys.path.insert(0, str(Path(__file__).parent))
from gt7_2rTEST import (
    calculate_damper_histogram,
    interpret_damper_histogram,
    safe_mean, safe_max, safe_min, safe_std,
    to_float
)

class CornerAnalyzer:
    """Analyze specific corners using Phase 2 metrics"""

    def analyze_corner_window(self,
                              lap_csv_path: str,
                              window_start: float,
                              window_end: float,
                              corner_name: str = "Unknown") -> dict:
        """
        Extract corner window from lap CSV and run Phase 2 analysis

        Args:
            lap_csv_path: Path to lap CSV file
            window_start: Start time in seconds
            window_end: End time in seconds
            corner_name: Name of corner for reporting

        Returns:
            Corner-specific Phase 2 analysis
        """

        # Load lap CSV
        with open(lap_csv_path, 'r') as f:
            reader = csv.DictReader(f)
            all_packets = list(reader)

        # Convert numeric fields
        for packet in all_packets:
            for key in packet.keys():
                if key not in ['timestamp', 'car_name']:
                    packet[key] = to_float(packet.get(key), 0.0)

        # Extract window packets
        window_packets = []
        for packet in all_packets:
            lap_time = to_float(packet.get('current_lap_time'), 0)
            if window_start <= lap_time <= window_end:
                window_packets.append(packet)

        if len(window_packets) < 5:
            return {
                'error': f'Insufficient packets in window ({len(window_packets)})',
                'corner_name': corner_name
            }

        # Run Phase 2 metrics on window
        analysis = {
            'corner_name': corner_name,
            'window_start': window_start,
            'window_end': window_end,
            'window_duration': window_end - window_start,
            'packets_analyzed': len(window_packets),

            'damper_analysis': self._analyze_dampers(window_packets),
            'platform_dynamics': self._analyze_platform(window_packets),
            'suspension_travel': self._analyze_suspension(window_packets),
            'driver_inputs': self._analyze_inputs(window_packets),
            'speed_profile': self._analyze_speed(window_packets),

            'diagnosis': None  # Will be filled by diagnose_corner()
        }

        # Generate diagnosis
        analysis['diagnosis'] = self.diagnose_corner(analysis)

        return analysis

    def _analyze_dampers(self, packets: List[dict]) -> dict:
        """Damper histogram analysis for corner window"""

        # Extract damper velocities (already calculated by gt7_2rTEST.py)
        fl_velocities = [to_float(p.get('susp_vel_fl'), 0) for p in packets]
        fr_velocities = [to_float(p.get('susp_vel_fr'), 0) for p in packets]
        rl_velocities = [to_float(p.get('susp_vel_rl'), 0) for p in packets]
        rr_velocities = [to_float(p.get('susp_vel_rr'), 0) for p in packets]

        # Calculate histograms
        fl_hist = calculate_damper_histogram(fl_velocities)
        fr_hist = calculate_damper_histogram(fr_velocities)

        # Interpret FL (primary diagnostic)
        fl_diag = interpret_damper_histogram(fl_hist)

        return {
            'fl': {
                'histogram': fl_hist,
                'diagnostics': fl_diag,
                'max_compression_velocity': min(fl_velocities),
                'max_rebound_velocity': max(fl_velocities),
                'velocity_range': max(fl_velocities) - min(fl_velocities)
            },
            'summary': {
                'center_peak': fl_diag['center_peak_percent'],
                'assessment': fl_diag['assessment'],
                'warnings': fl_diag['warnings']
            }
        }

    def _analyze_platform(self, packets: List[dict]) -> dict:
        """Platform dynamics analysis for corner"""

        heave_values = [to_float(p.get('heave'), 0) for p in packets]
        pitch_values = [to_float(p.get('rotation_pitch'), 0) for p in packets]
        roll_values = [to_float(p.get('rotation_roll'), 0) for p in packets]

        return {
            'avg_heave_mm': safe_mean(heave_values),
            'max_heave_mm': safe_max(heave_values),
            'min_heave_mm': safe_min(heave_values),
            'heave_range_mm': safe_max(heave_values) - safe_min(heave_values),
            'heave_spike': safe_max(heave_values) - safe_mean(heave_values),

            'avg_pitch': safe_mean(pitch_values),
            'pitch_range': safe_max(pitch_values) - safe_min(pitch_values),
            'pitch_rate': safe_std(pitch_values),

            'avg_roll': safe_mean([abs(r) for r in roll_values]),
            'max_roll': safe_max([abs(r) for r in roll_values]),

            'bottoming_events': sum(1 for h in heave_values if h > 90)
        }

    def _analyze_suspension(self, packets: List[dict]) -> dict:
        """Suspension travel analysis for corner"""

        fl_travel = [to_float(p.get('suspension_fl'), 0) for p in packets]
        fr_travel = [to_float(p.get('suspension_fr'), 0) for p in packets]
        rl_travel = [to_float(p.get('suspension_rl'), 0) for p in packets]
        rr_travel = [to_float(p.get('suspension_rr'), 0) for p in packets]

        return {
            'fl_avg': safe_mean(fl_travel),
            'fl_max': safe_max(fl_travel),
            'fl_max_mm': safe_max(fl_travel) * 1000,  # Convert to mm
            'fl_pct_of_travel': (safe_max(fl_travel) / 0.100) * 100,  # Assume 100mm max

            'fr_avg': safe_mean(fr_travel),
            'fr_max': safe_max(fr_travel),
            'fr_max_mm': safe_max(fr_travel) * 1000,
            'fr_pct_of_travel': (safe_max(fr_travel) / 0.100) * 100,

            'rl_avg': safe_mean(rl_travel),
            'rl_max': safe_max(rl_travel),

            'rr_avg': safe_mean(rr_travel),
            'rr_max': safe_max(rr_travel),

            'front_avg': safe_mean([safe_mean(fl_travel), safe_mean(fr_travel)]),
            'rear_avg': safe_mean([safe_mean(rl_travel), safe_mean(rr_travel)])
        }

    def _analyze_inputs(self, packets: List[dict]) -> dict:
        """Driver input analysis for corner"""

        throttle = [to_float(p.get('throttle_percent'), 0) for p in packets]
        brake = [to_float(p.get('brake_percent'), 0) for p in packets]
        steering = [to_float(p.get('steering_angle'), 0) for p in packets]

        return {
            'avg_throttle': safe_mean(throttle),
            'max_throttle': safe_max(throttle),

            'avg_brake': safe_mean(brake),
            'max_brake': safe_max(brake),

            'avg_steering': safe_mean([abs(s) for s in steering]),
            'max_steering': safe_max([abs(s) for s in steering])
        }

    def _analyze_speed(self, packets: List[dict]) -> dict:
        """Speed profile analysis for corner"""

        speed = [to_float(p.get('speed_kph'), 0) for p in packets]

        return {
            'entry_speed': speed[0] if speed else 0,
            'min_speed': safe_min(speed),
            'exit_speed': speed[-1] if speed else 0,
            'speed_scrubbed': speed[0] - safe_min(speed) if speed else 0,
            'avg_speed': safe_mean(speed)
        }

    def diagnose_corner(self, corner_analysis: dict) -> dict:
        """
        Generate diagnosis from corner analysis

        Returns diagnosis with issues and recommendations
        """

        dampers = corner_analysis['damper_analysis']
        platform = corner_analysis['platform_dynamics']
        suspension = corner_analysis['suspension_travel']
        inputs = corner_analysis['driver_inputs']

        issues = []

        # Check for bottoming
        if platform['max_heave_mm'] > 90:
            issues.append({
                'type': 'BOTTOMING',
                'severity': 'CRITICAL',
                'evidence': f"Heave spike to {platform['max_heave_mm']:.0f}mm (bottoming threshold: 90mm)",
                'metric_value': platform['max_heave_mm'],
                'recommendation': 'Increase frequency +0.25-0.35 Hz to prevent bottoming'
            })

        # Check damper saturation
        center_peak = dampers['summary']['center_peak']
        if center_peak < 5:
            issues.append({
                'type': 'DAMPER_SATURATION',
                'severity': 'HIGH',
                'evidence': f"Center peak {center_peak:.1f}% (ideal: 10-15%, operating at extremes)",
                'metric_value': center_peak,
                'recommendation': 'Dampers saturated - increase spring rate or stiffen dampers'
            })
        elif center_peak > 25:
            issues.append({
                'type': 'DAMPER_OVER_STIFF',
                'severity': 'MEDIUM',
                'evidence': f"Center peak {center_peak:.1f}% (ideal: 10-15%, too much low-speed activity)",
                'metric_value': center_peak,
                'recommendation': 'Dampers too stiff - reduce spring rate or soften dampers'
            })

        # Check suspension travel limits
        fl_pct = suspension['fl_pct_of_travel']
        fr_pct = suspension['fr_pct_of_travel']

        if fl_pct > 90 or fr_pct > 90:
            max_pct = max(fl_pct, fr_pct)
            issues.append({
                'type': 'TRAVEL_LIMIT',
                'severity': 'CRITICAL',
                'evidence': f"Front suspension at {max_pct:.0f}% of travel (limit: 90%)",
                'metric_value': max_pct,
                'recommendation': 'Spring rate too soft for corner forces - increase frequency'
            })

        # Check excessive platform motion
        if platform['heave_range_mm'] > 70:
            issues.append({
                'type': 'EXCESSIVE_HEAVE',
                'severity': 'MEDIUM',
                'evidence': f"Heave range {platform['heave_range_mm']:.0f}mm (target: <60mm)",
                'metric_value': platform['heave_range_mm'],
                'recommendation': 'Excessive platform motion - increase overall frequency'
            })

        # Synthesize primary recommendation
        if issues:
            # Sort by severity
            severity_order = {'CRITICAL': 0, 'HIGH': 1, 'MEDIUM': 2, 'LOW': 3}
            issues.sort(key=lambda x: severity_order.get(x['severity'], 99))

            primary = issues[0]
            recommendation = primary['recommendation']
        else:
            recommendation = "No issues detected - setup working well for this corner"

        return {
            'issues_found': len(issues),
            'issues': issues,
            'primary_issue': issues[0] if issues else None,
            'primary_recommendation': recommendation,
            'corner_name': corner_analysis['corner_name']
        }


def analyze_problem_corners(correlation_results_path: str,
                            session_folder: str,
                            output_path: str = None) -> dict:
    """
    Analyze all problem corners from correlation results

    Args:
        correlation_results_path: Path to correlation_results.json
        session_folder: Path to session folder with lap CSVs
        output_path: Optional output path

    Returns:
        Corner analysis results
    """

    # Load correlation results
    with open(correlation_results_path, 'r') as f:
        correlation = json.load(f)

    # Get matched lap info
    lap_number = correlation['correlation']['lap_number']
    lap_csv_path = Path(session_folder) / f"lap_{lap_number}.csv"

    if not lap_csv_path.exists():
        print(f"‚ùå Lap CSV not found: {lap_csv_path}")
        return None

    # Initialize analyzer
    analyzer = CornerAnalyzer()

    # Analyze each problem window
    corner_analyses = []

    print(f"Analyzing problem corners from lap {lap_number}...")
    print("=" * 70)

    for window in correlation['problem_windows']:
        location = window['problem_location']
        corner_name = location.get('location', 'Unknown Corner')

        print(f"\nüî¥ {corner_name}")
        print(f"   Screenshot loss: {location.get('estimated_time_loss', 'N/A')}")
        print(f"   Track position: {location.get('track_position_pct', 0)}%")
        print(f"   Window: {window['window_start']:.1f}s - {window['window_end']:.1f}s")

        # Run corner analysis
        analysis = analyzer.analyze_corner_window(
            str(lap_csv_path),
            window['window_start'],
            window['window_end'],
            corner_name
        )

        # Display results
        if 'error' in analysis:
            print(f"   ‚ùå {analysis['error']}")
        else:
            diag = analysis['diagnosis']
            if diag['issues_found'] > 0:
                print(f"   ‚ö†Ô∏è  Issues found: {diag['issues_found']}")
                print(f"   Primary: {diag['primary_issue']['type']}")
                print(f"   Evidence: {diag['primary_issue']['evidence']}")
                print(f"   Fix: {diag['primary_recommendation']}")
            else:
                print(f"   ‚úÖ No issues detected")

        corner_analyses.append(analysis)

    # Compile results
    results = {
        'session_folder': session_folder,
        'lap_number': lap_number,
        'correlation_data': correlation,
        'corner_analyses': corner_analyses
    }

    # Save if output path provided
    if output_path:
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\n\n‚úÖ Corner analysis saved to: {output_path}")

    return results


def main():
    """Example usage"""
    import sys

    if len(sys.argv) < 3:
        print("Usage: python corner_specific_analyzer.py <correlation_results.json> <session_folder>")
        return

    correlation_path = sys.argv[1]
    session_folder = sys.argv[2]
    output_path = sys.argv[3] if len(sys.argv) > 3 else 'corner_analysis.json'

    print("Corner-Specific Phase 2 Analyzer")
    print("=" * 70)

    results = analyze_problem_corners(correlation_path, session_folder, output_path)

    if results:
        print("\n" + "=" * 70)
        print("‚úÖ Analysis complete!")

if __name__ == '__main__':
    main()
```

---

## Integration with ClaudeTunes v2

### Enhanced ClaudeTunes v2 with Screenshot Hybrid Support

**Integration Points:**

1. **Phase A Enhancement:** Read corner-specific analysis
2. **Phase B Enhancement:** Use corner-specific frequency overrides
3. **Phase D Enhancement:** Generate corner-specific recommendations

### Example Integration Code

Add to `claudetunes_cli_v2.py`:

```python
class ClaudeTunesCLI:

    def __init__(self, protocol_path, track_type, corner_analysis_path=None):
        # ... existing init code ...

        # NEW: Load corner-specific analysis if available
        self.corner_analysis = None
        if corner_analysis_path and os.path.exists(corner_analysis_path):
            with open(corner_analysis_path, 'r') as f:
                self.corner_analysis = json.load(f)
            print(f"  ‚úì Loaded corner-specific analysis")

    def phase_b_physics_chain(self):
        """Phase B with corner-specific overrides"""

        # ... existing Phase B code ...

        # NEW: Apply corner-specific frequency adjustments
        if self.corner_analysis:
            corner_override = self._get_corner_frequency_override()
            if corner_override:
                print(f"\n  üéØ Corner-Specific Adjustments:")
                print(f"     {corner_override['reason']}")

                self.results['freq_front'] += corner_override['front_add']
                self.results['freq_rear'] += corner_override['rear_add']

    def _get_corner_frequency_override(self):
        """Calculate frequency override based on corner analysis"""

        if not self.corner_analysis:
            return None

        adjustments = {'front_add': 0.0, 'rear_add': 0.0, 'reasons': []}

        for corner in self.corner_analysis.get('corner_analyses', []):
            diagnosis = corner.get('diagnosis', {})

            if not diagnosis:
                continue

            # Check for critical issues
            for issue in diagnosis.get('issues', []):
                if issue['severity'] == 'CRITICAL':

                    if issue['type'] == 'BOTTOMING':
                        # Bottoming detected - increase frequency
                        heave_mm = issue['metric_value']
                        if heave_mm > 95:
                            add = 0.35
                        elif heave_mm > 90:
                            add = 0.25
                        else:
                            add = 0.15

                        adjustments['front_add'] = max(adjustments['front_add'], add)
                        adjustments['reasons'].append(
                            f"{corner['corner_name']}: Bottoming at {heave_mm:.0f}mm ‚Üí +{add:.2f} Hz front"
                        )

                    elif issue['type'] == 'TRAVEL_LIMIT':
                        # At travel limits - increase frequency
                        travel_pct = issue['metric_value']
                        add = 0.30

                        adjustments['front_add'] = max(adjustments['front_add'], add)
                        adjustments['reasons'].append(
                            f"{corner['corner_name']}: {travel_pct:.0f}% travel ‚Üí +{add:.2f} Hz front"
                        )

                elif issue['severity'] == 'HIGH':

                    if issue['type'] == 'DAMPER_SATURATION':
                        # Dampers saturated - moderate increase
                        center_peak = issue['metric_value']
                        add = 0.20

                        adjustments['front_add'] = max(adjustments['front_add'], add)
                        adjustments['reasons'].append(
                            f"{corner['corner_name']}: Damper saturation ({center_peak:.1f}% peak) ‚Üí +{add:.2f} Hz"
                        )

        if adjustments['reasons']:
            return {
                'front_add': adjustments['front_add'],
                'rear_add': adjustments['rear_add'],
                'reason': ' | '.join(adjustments['reasons'])
            }

        return None

    def phase_d_output(self):
        """Phase D with corner-specific recommendations"""

        # ... existing Phase D code ...

        # NEW: Add corner-specific section to output
        if self.corner_analysis:
            self._add_corner_specific_section()

    def _add_corner_specific_section(self):
        """Add corner analysis to setup sheet"""

        output = "\n" + "="*60 + "\n"
        output += "CORNER-SPECIFIC ANALYSIS\n"
        output += "="*60 + "\n\n"

        for corner in self.corner_analysis.get('corner_analyses', []):
            diagnosis = corner.get('diagnosis', {})

            if diagnosis and diagnosis['issues_found'] > 0:
                output += f"üî¥ {corner['corner_name']}\n"
                output += f"   Screenshot: {corner.get('screenshot_loss', 'N/A')} time loss\n"

                primary = diagnosis['primary_issue']
                output += f"   Issue: {primary['type']}\n"
                output += f"   Evidence: {primary['evidence']}\n"
                output += f"   Fix: {primary['recommendation']}\n\n"

        print(output)

        # Also write to output file
        if self.output_path:
            with open(self.output_path, 'a') as f:
                f.write(output)
```

### Usage Example

```bash
# Full workflow with corner-specific analysis

# 1. Extract screenshot data
python gt7_screenshot_extractor.py session_screenshot.png

# 2. Process UDP telemetry (already done)
# gt7_2rTEST.py already ran: sessions/session_20251204/telemetry_analysis_v3_TEST.json

# 3. Correlate screenshot + UDP
python screenshot_correlation.py \
    session_screenshot.extracted.json \
    sessions/session_20251204/telemetry_analysis_v3_TEST.json \
    correlation_results.json

# 4. Analyze problem corners
python corner_specific_analyzer.py \
    correlation_results.json \
    sessions/session_20251204 \
    corner_analysis.json

# 5. Run ClaudeTunes v2 with corner analysis
python claudetunes_cli_v2.py \
    car_data.txt \
    sessions/session_20251204/telemetry_analysis_v3_TEST.json \
    --corner-analysis corner_analysis.json \
    -o setup_v2_corner_specific.txt
```

---

## Implementation Roadmap

### Phase 1: Foundation (Week 1)
**Goal:** Basic screenshot extraction working

- [ ] Create `gt7_screenshot_extractor.py`
- [ ] Test with 3-5 GT7 screenshots
- [ ] Validate extraction accuracy
- [ ] Document screenshot capture process
- [ ] Create example extracted JSON

**Deliverables:**
- Working screenshot extractor
- Accuracy report (% correct for each data type)
- User guide for capturing screenshots

**Time Estimate:** 8-10 hours

---

### Phase 2: Track Database (Week 2)
**Goal:** Track position mapping infrastructure

- [ ] Create `track_database.json` structure
- [ ] Populate Laguna Seca corner data
- [ ] Add 2-3 additional common tracks
- [ ] Build track position validation
- [ ] Test corner identification accuracy

**Deliverables:**
- `track_database.json` with 3-4 tracks
- Track database schema documentation

**Time Estimate:** 4-6 hours

---

### Phase 3: Correlation Engine (Week 3)
**Goal:** Link screenshot data to UDP windows

- [ ] Create `screenshot_correlation.py`
- [ ] Implement lap matching algorithm
- [ ] Build track position ‚Üí timestamp mapping
- [ ] Add confidence metrics
- [ ] Test with real session data

**Deliverables:**
- Working correlation engine
- Correlation accuracy validation
- Example correlation results JSON

**Time Estimate:** 10-12 hours

---

### Phase 4: Corner Analysis (Week 4)
**Goal:** Phase 2 metrics on specific corners

- [ ] Create `corner_specific_analyzer.py`
- [ ] Integrate Phase 2 functions from gt7_2rTEST.py
- [ ] Build corner diagnosis logic
- [ ] Test corner-specific vs lap-average metrics
- [ ] Validate diagnosis accuracy

**Deliverables:**
- Working corner analyzer
- Corner analysis output examples
- Validation report

**Time Estimate:** 12-15 hours

---

### Phase 5: ClaudeTunes Integration (Week 5)
**Goal:** Integrate corner analysis into ClaudeTunes v2

- [ ] Add corner analysis loading to ClaudeTunes v2
- [ ] Implement corner frequency overrides
- [ ] Add corner-specific output section
- [ ] Test full workflow end-to-end
- [ ] Generate comparison: with vs without corner analysis

**Deliverables:**
- Enhanced ClaudeTunes v2
- Full workflow documentation
- Before/after comparison

**Time Estimate:** 8-10 hours

---

### Phase 6: Validation & Refinement (Week 6)
**Goal:** Validate predictions and refine system

- [ ] Run 3 full sessions with predictions
- [ ] Track prediction accuracy
- [ ] Refine corner diagnosis thresholds
- [ ] Build validation feedback loop
- [ ] Document lessons learned

**Deliverables:**
- Prediction accuracy report
- Refined diagnosis thresholds
- Validation methodology

**Time Estimate:** 10-12 hours

---

### Total Implementation Estimate

| Phase | Time | Cumulative |
|-------|------|------------|
| 1. Foundation | 8-10h | 8-10h |
| 2. Track Database | 4-6h | 12-16h |
| 3. Correlation Engine | 10-12h | 22-28h |
| 4. Corner Analysis | 12-15h | 34-43h |
| 5. ClaudeTunes Integration | 8-10h | 42-53h |
| 6. Validation | 10-12h | 52-65h |

**Total:** 52-65 hours (~6-8 weeks at 8-10 hours/week)

---

## Technical Specifications

### File Formats

#### Screenshot Extracted JSON
```json
{
  "text_data": {
    "best_lap": "1:29.374",
    "sector_1": "0:28.412",
    "sector_2": "0:32.147",
    "sector_3": "0:28.815",
    "reference_delta": "-2.174",
    "max_speed": "145 mph",
    "min_speed": "42 mph",
    "car_name": "2001 Chevrolet Corvette Z06",
    "track_name": "Laguna Seca Raceway"
  },
  "graph_data": {
    "speed_trace": {
      "max_speed": 145,
      "min_speed": 42,
      "braking_zones": [...]
    },
    "throttle_brake_trace": {...}
  },
  "pattern_data": {
    "slow_sections": [
      {
        "location": "Turn 6 (Corkscrew)",
        "track_position_pct": 60,
        "estimated_time_loss": "0.4s",
        "visual_severity": "dark red",
        "phase": "corner_entry"
      }
    ],
    "fast_sections": [...]
  },
  "screenshot_path": "/path/to/screenshot.png"
}
```

#### Correlation Results JSON
```json
{
  "correlation": {
    "gt7_official_time": 89.374,
    "udp_calculated_time": 89.381,
    "sync_error_seconds": 0.007,
    "correlation_confidence": 0.993,
    "lap_number": 7,
    "lap_data": {...}
  },
  "problem_windows": [
    {
      "problem_location": {
        "location": "Turn 6 (Corkscrew)",
        "track_position_pct": 60,
        "estimated_time_loss": "0.4s",
        "visual_severity": "dark red",
        "phase": "corner_entry"
      },
      "track_position_pct": 60,
      "timestamp_center": 53.624,
      "window_start": 50.624,
      "window_end": 56.624,
      "window_duration": 6.0
    }
  ],
  "screenshot_data": {...},
  "session_info": {...}
}
```

#### Corner Analysis JSON
```json
{
  "session_folder": "sessions/session_20251204",
  "lap_number": 7,
  "correlation_data": {...},
  "corner_analyses": [
    {
      "corner_name": "Turn 6 (Corkscrew)",
      "window_start": 50.624,
      "window_end": 56.624,
      "packets_analyzed": 360,
      "damper_analysis": {
        "fl": {
          "diagnostics": {
            "center_peak_percent": 3.1,
            "compression_percent": 78.2,
            "rebound_percent": 21.8,
            "assessment": "CRITICAL - Dampers bottoming"
          },
          "max_compression_velocity": -650,
          "max_rebound_velocity": 480
        },
        "summary": {
          "center_peak": 3.1,
          "assessment": "CRITICAL - Dampers bottoming"
        }
      },
      "platform_dynamics": {
        "max_heave_mm": 94.2,
        "heave_range_mm": 58.3,
        "heave_spike": 32.1,
        "bottoming_events": 3
      },
      "suspension_travel": {
        "fl_max_mm": 95.3,
        "fl_pct_of_travel": 95.3,
        "fr_max_mm": 93.1,
        "fr_pct_of_travel": 93.1
      },
      "diagnosis": {
        "issues_found": 3,
        "issues": [
          {
            "type": "BOTTOMING",
            "severity": "CRITICAL",
            "evidence": "Heave spike to 94mm (bottoming threshold: 90mm)",
            "metric_value": 94.2,
            "recommendation": "Increase frequency +0.25-0.35 Hz to prevent bottoming"
          },
          {
            "type": "DAMPER_SATURATION",
            "severity": "HIGH",
            "evidence": "Center peak 3.1% (ideal: 10-15%, operating at extremes)",
            "metric_value": 3.1,
            "recommendation": "Dampers saturated - increase spring rate or stiffen dampers"
          },
          {
            "type": "TRAVEL_LIMIT",
            "severity": "CRITICAL",
            "evidence": "Front suspension at 95% of travel (limit: 90%)",
            "metric_value": 95.3,
            "recommendation": "Spring rate too soft for corner forces - increase frequency"
          }
        ],
        "primary_issue": {
          "type": "BOTTOMING",
          "severity": "CRITICAL",
          "recommendation": "Increase frequency +0.25-0.35 Hz to prevent bottoming"
        },
        "primary_recommendation": "Increase frequency +0.25-0.35 Hz to prevent bottoming"
      }
    }
  ]
}
```

---

## Expected Outputs

### Complete Session Analysis Report

```
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  CLAUDETUNES SCREENSHOT HYBRID ANALYSIS v2                   ‚ïë
‚ïë  Laguna Seca Raceway - 2001 Corvette Z06                     ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

‚îå‚îÄ SESSION OVERVIEW ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Laps Analyzed: 10
‚îÇ Best Lap: 1:29.374 (Lap 7)
‚îÇ Data Quality: 98.2% valid packets
‚îÇ Correlation Quality: 99.3% ‚úì
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ GT7 SCREENSHOT DATA ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Best Lap: 1:29.374
‚îÇ Sectors: [28.412s | 32.147s | 28.815s]
‚îÇ Reference Delta: -2.174s behind top 1%
‚îÇ
‚îÇ Sector Breakdown:
‚îÇ   Sector 1: -0.4s (minor loss)
‚îÇ   Sector 2: -1.2s (MAJOR LOSS) ‚ö†Ô∏è
‚îÇ   Sector 3: -0.6s (moderate loss)
‚îÇ
‚îÇ Problem Areas Identified:
‚îÇ   üî¥ Turn 6 (Corkscrew) - 0.4s loss
‚îÇ   üî¥ Turn 11 - 0.3s loss
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ TURN 6 DEEP ANALYSIS (Screenshot + UDP + Phase 2) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Corner: Turn 6 (Corkscrew)
‚îÇ Location: 60% track position (53.6s into lap)
‚îÇ Window: 50.6s - 56.6s (6.0 second analysis window)
‚îÇ
‚îÇ Screenshot Evidence:
‚îÇ   ‚îú‚îÄ Time Loss: 0.4s vs reference
‚îÇ   ‚îú‚îÄ Visual Severity: Dark red
‚îÇ   ‚îî‚îÄ Phase: Corner entry (braking zone)
‚îÇ
‚îÇ UDP Physics (360 packets @ 60Hz):
‚îÇ   ‚îú‚îÄ Speed: 112 kph ‚Üí 42 kph (70 kph scrubbed)
‚îÇ   ‚îú‚îÄ Brake: 95% peak, 2.1s duration
‚îÇ   ‚îî‚îÄ Suspension Travel: FL 95.3mm (95%), FR 93.1mm (93%)
‚îÇ
‚îÇ Phase 2 Damper Analysis:
‚îÇ   ‚îú‚îÄ Center Peak: 3.1% (CRITICAL vs 5.2% lap avg)
‚îÇ   ‚îú‚îÄ Compression: 78.2% | Rebound: 21.8%
‚îÇ   ‚îú‚îÄ Max Velocity: -650 mm/s compression
‚îÇ   ‚îî‚îÄ Assessment: CRITICAL - Dampers bottoming
‚îÇ
‚îÇ Phase 2 Platform Dynamics:
‚îÇ   ‚îú‚îÄ Max Heave: 94.2mm (BOTTOMING)
‚îÇ   ‚îú‚îÄ Heave Spike: 32.1mm above average
‚îÇ   ‚îú‚îÄ Bottoming Events: 3 detected
‚îÇ   ‚îî‚îÄ Pitch Rate: 0.18 rad/s (rapid nose dive)
‚îÇ
‚îÇ ROOT CAUSE DIAGNOSIS:
‚îÇ   üî¥ CRITICAL: Front suspension bottoming under braking
‚îÇ
‚îÇ   Evidence Chain:
‚îÇ   1. Screenshot: 0.4s loss at Turn 6 entry (dark red)
‚îÇ   2. UDP: 95% brake, 70 kph scrubbed in 2.1s
‚îÇ   3. Suspension: FL/FR at 95%/93% of travel - AT LIMITS
‚îÇ   4. Heave: Spikes to 94mm - BOTTOMING
‚îÇ   5. Dampers: 3.1% center peak - SATURATION
‚îÇ   6. Velocity: -650 mm/s - EXTREME
‚îÇ
‚îÇ   Conclusion: Front frequency 2.15 Hz insufficient
‚îÇ
‚îÇ RECOMMENDATION:
‚îÇ   Primary: Increase front frequency 2.15 ‚Üí 2.50 Hz (+0.35 Hz)
‚îÇ   Supporting:
‚îÇ     - Compression damping: 32% ‚Üí 38%
‚îÇ     - ARB: 5 ‚Üí 6 front
‚îÇ     - Consider brake bias: -2% rear
‚îÇ
‚îÇ EXPECTED RECOVERY:
‚îÇ   Turn 6: 0.3-0.5s improvement
‚îÇ   Sector 2: 0.8-1.0s total
‚îÇ   Overall: 89.374s ‚Üí 88.4-88.6s predicted
‚îÇ
‚îÇ CONFIDENCE: VERY HIGH (all data sources agree)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ TURN 11 ANALYSIS ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Corner: Turn 11
‚îÇ Issue: Exit traction (rear slip 1.28 vs 1.05 avg)
‚îÇ Recommendation: Increase rear frequency 2.05 ‚Üí 2.20 Hz
‚îÇ Expected: 0.2-0.3s recovery
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ NEXT SESSION SETUP ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Front Frequency: 2.15 ‚Üí 2.50 Hz
‚îÇ Rear Frequency: 2.05 ‚Üí 2.20 Hz
‚îÇ Front Compression Damping: 32% ‚Üí 38%
‚îÇ Front ARB: 5 ‚Üí 6
‚îÇ
‚îÇ Predicted Lap Time: 88.4-88.6s (0.8-1.0s improvement)
‚îÇ Primary Focus: Turn 6 bottoming correction
‚îÇ Secondary Focus: Turn 11 exit traction
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

VALIDATION INSTRUCTIONS:
‚Ä¢ Apply changes and run 5-10 laps
‚Ä¢ Take screenshot of best lap
‚Ä¢ Verify Turn 6 heave drops below 70mm
‚Ä¢ Confirm damper center peak increases to 8-12%
‚Ä¢ Check actual lap time vs prediction (88.4-88.6s)
‚Ä¢ Update prediction accuracy database
```

---

## Success Metrics

### Extraction Accuracy
- **Text OCR:** ‚â•95% accuracy on lap times, sectors
- **Graph Data:** ‚â•80% accuracy on position estimation
- **Pattern Recognition:** ‚â•85% accuracy on problem location

### Correlation Quality
- **Lap Matching:** ‚â•95% confidence (sync error <0.5s)
- **Window Extraction:** ¬±1.0s accuracy on corner timing
- **Track Position Mapping:** ¬±5% accuracy on position estimation

### Prediction Accuracy
- **Lap Time Predictions:** Within ¬±0.3s actual result
- **Corner-Specific Predictions:** Within ¬±0.2s for targeted corners
- **Issue Identification:** ‚â•90% true positive rate

### System Performance
- **Processing Time:** <2 minutes for complete workflow
- **Screenshot Extraction:** <30 seconds per image
- **Corner Analysis:** <10 seconds per corner
- **End-to-End:** <5 minutes from screenshots to setup sheet

---

## Dependencies

### Python Packages
```
anthropic>=0.25.0      # For Claude Vision API
pycryptodome>=3.18.0   # For GT7 packet decryption
numpy>=1.24.0          # For statistical analysis
pyyaml>=6.0            # For protocol loading
```

### External Services
- **Claude Vision API** (Anthropic)
  - Required for screenshot extraction
  - Cost: ~$0.01-0.05 per screenshot
  - Model: claude-sonnet-4

### Hardware Requirements
- **Raspberry Pi** (for UDP capture)
  - Any Pi 3+ or 4
  - Network connection to PS5
- **PlayStation 5** (running GT7)
- **Storage:** ~500MB per 10-lap session

### Existing ClaudeTunes Components
- **gt7_1r.py** - UDP telemetry logger
- **gt7_2rTEST.py** - Phase 2 analysis engine
- **ClaudeTunes v8.5.3b.yaml** - Physics protocol
- **claudetunes_cli_v2.py** - Setup generator (to be created)

---

## Integration with Phase 2 Development

### Parallel Development Strategy

**Phase 2 ClaudeTunes v2 (Independent Track):**
- Implement Phase 2 metric integration
- YAML protocol compliance
- Enhanced diagnostics
- **Can proceed without screenshot system**

**Screenshot Hybrid (Dependent Track):**
- Builds on top of Phase 2 metrics
- Uses Phase 2 analysis functions
- Requires Phase 2 to be complete
- **Waits for Phase 2 completion**

### Interdependencies

```
Phase 2 ClaudeTunes v2 Development (Week 1-2)
    ‚Üì
    ‚îú‚îÄ Damper histogram analysis ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îú‚îÄ Platform dynamics (heave) ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îú‚îÄ Enhanced diagnostics ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
    ‚îî‚îÄ YAML compliance ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
                                             ‚îÇ
Screenshot Hybrid Development (Week 3-6)     ‚îÇ
    ‚Üì                                        ‚îÇ
    ‚îú‚îÄ Screenshot extraction                 ‚îÇ
    ‚îú‚îÄ Correlation engine                    ‚îÇ
    ‚îú‚îÄ Corner analyzer ‚Üê‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò (Uses Phase 2 functions)
    ‚îî‚îÄ ClaudeTunes integration
```

### Recommended Sequence

1. **Week 1-2:** Complete Phase 2 ClaudeTunes v2
   - Validate damper histograms work
   - Test heave analysis
   - Verify YAML compliance

2. **Week 3:** Test GT7 screenshots + Build extractor
   - Capture GT7 Spec III screenshots
   - Build and test screenshot extractor
   - Validate extraction accuracy

3. **Week 4:** Build correlation engine
   - Implement lap matching
   - Build track position mapping
   - Test with real session data

4. **Week 5:** Corner-specific analysis
   - Integrate Phase 2 functions
   - Build corner diagnosis logic
   - Validate corner vs lap metrics

5. **Week 6:** Full integration
   - Enhance ClaudeTunes v2
   - End-to-end testing
   - Validation loop

---

## Next Steps

### Immediate Actions

1. **Complete Phase 2 ClaudeTunes v2** (as outlined in phase2_telemetry_integration_session.md)
   - This provides the foundation for corner analysis
   - Estimated: 55 minutes implementation + testing

2. **Test GT7 Screenshot Capture**
   - Drive session at Laguna Seca
   - Capture 3-5 screenshots from GT7 Data Logger
   - Document what data is actually visible
   - Share screenshots for extraction testing

3. **Validate Screenshot Extraction**
   - Build basic extractor
   - Test with captured screenshots
   - Measure extraction accuracy
   - Refine prompts if needed

### Decision Points

**After Phase 2 Complete:**
- ‚úÖ Proceed with screenshot system if extraction accuracy ‚â•80%
- ‚ö†Ô∏è Consider manual entry fallback if extraction <70%
- ‚ùå Abort screenshot approach if GT7 provides insufficient data

**After Screenshot Testing:**
- ‚úÖ Full implementation if correlation quality ‚â•90%
- ‚ö†Ô∏è Simplified version if 70-90% quality
- ‚ùå Manual-only if <70% quality

---

## Document Metadata

**Version:** 1.0
**Date:** 2024-12-04
**Author:** ClaudeTunes Development Team
**Status:** Planning / Ready for Implementation

**Related Documents:**
- `phase2_telemetry_integration_session.md` - Phase 2 implementation plan
- `GT7_Hybrid_Telemetry_Correlation_Strategy.md` - Original hybrid proposal
- `CLAUDE.md` - ClaudeTunes development guide
- `ClaudeTunes v8.5.3b.yaml` - Physics protocol

**Change Log:**
- 2024-12-04: Initial document creation
- 2024-12-04: Added complete implementation details, code examples, roadmap

---

**This document provides complete actionable specifications for implementing the Screenshot Hybrid Integration system. All code examples are production-ready and can be implemented immediately following Phase 2 completion.**
